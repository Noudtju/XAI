{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PIP-Net Installation Guide**\n",
    "\n",
    "PIP-Net (Patch-based Intuitive Prototypes Network) is an interpretable and intuitive deep learning method for image classification. PIP-Net learns prototypical parts: interpretable concepts visualized as image patches. PIP-Net classifies an image with a sparse scoring sheet where the presence of a prototypical part in an image adds evidence for a class. PIP-Net is globally interpretable since the set of learned prototypes shows the entire reasoning of the model. A smaller local explanation locates the relevant prototypes in a test image. The model can also abstain from a decision for out-of-distribution data by saying “I haven’t seen this before”. The model only uses image-level labels and does not rely on any part annotations.\n",
    "\n",
    "Code is open-sourced and can be found here: https://github.com/M-Nauta/PIPNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PIP-Net for Birds and Cars**\n",
    "\n",
    "In this tutorial, you'll play with PIP-Net trained on birds or cars. In your project, you could for example learn people to become a birdwatcher with the explanations of PIP-Net. Of  course you can also train PIP-Net on another dataset. You can train PIP-Net yourselves and in case of difficulties, please contact me: m.nauta@datacation.nl I can also make a PIP-Net model trained on cats/dogs available: let me know your interest.\n",
    "\n",
    "You can download a preprocessed version of the birds dataset via [Google Drive](https://drive.google.com/file/d/1PfeW5afu3cSdWTi03Ac3PYH8O1BSp9kK/view?usp=sharing) (use your tilburg university account!). \n",
    "\n",
    "Download the zip-folder on [Google Drive](https://drive.google.com/file/d/195oPh4-ugl8LkqFrlPwzHaZ3rH16Jc7a/view?usp=sharing) containing a checkpoint of PIP-Net trained on the birds dataset and all visualisations you might want to use: global explanations and local explanations. Use your tilburg university account for downloading! You are not allowed to share these files with others, only for educational purposes for JADS. In case you use Github, make sure to set your repository to private.\n",
    "\n",
    "For the CARS dataset, download dataset+visualisations+checkpoint via [Google Drive](https://drive.google.com/file/d/1Pnmo7tAFZDpePjyQQT-6YOLA-H8rnKG8/view?usp=sharing) (use your Tilburg university account!). You are not allowed to share these files with others, only with your group for educational purposes for JADS. In case you use Github, make sure to set your repository to private. \n",
    "\n",
    "\n",
    "https://github.com/M-Nauta/PIPNet?tab=readme-ov-file#interpreting-the-results explains what the content of some folders/files mean. As you can see, PIP-Net is currently just a folder- and image-based explanation. This can be improved of course in some interactive way: that's up to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation**\n",
    "\n",
    "Create a virtual environment with uv (recommended new package manager which we also use at Datacation). Alternatives are Anaconda, pip or poetry. uv manages all your packages, and you can install packages within a virtual environment to prevent influencing other installations. Follow the installation tips here https://docs.astral.sh/uv/, and create a virtual environment with a name you can choose. Activate your virtual environment and add Python and numpy (if not already done during creation). For example:\n",
    "\n",
    "``conda create -n pipnetenv python numpy``\n",
    "\n",
    "Next, we'll start with installing PyTorch. PyTorch is a package for AI and specifically neural networks. PIP-Net code is written in PyTorch1.13 but PyTorch2 should be compatible. However, if you want to be sure you can also download PyTorch 1.13 via https://pytorch.org/get-started/previous-versions/#v1131.\n",
    "Go to https://pytorch.org/get-started/locally/, select the suited options and run the generated command. \n",
    "\" Compute platform\": if you have a cuda-compatible GPU on your device, you can select CUDA11.8 (not all packages support CUDA12 yet so best to choose 11.8.) If you don't have a cuda-compatible GPU, then choose CPU. \n",
    "Already have PyTorch on your system? Then you may also use that. \n",
    "\n",
    "Add the other required python packages as described in the README of https://github.com/M-Nauta/PIPNet to your virtual environment . "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose your trade-off: as-is or train PIP-Net**\n",
    "There are various degrees of freedom in this project:\n",
    "\n",
    "- If you do not have experience with neural networks, have not worked with PyTorch yet and would like to simply apply PIP-Net without having to understand the code: this is possible. Simply take the visualised prototypes as-is and take it from there. E.g., create a better way to present the explanation to the user. \n",
    "- You do want to make some changes to PIP-Net, for example to disable a prototype? You don't need to understand neural networks and the code in detail, but some basic knowledge of PyTorch is useful. And actually, I can recommend to learn a bit anyway to become an even better data scientist :). \n",
    "To learn the basics of PyTorch, check out https://pytorch.org/tutorials/beginner/basics/intro.html or watch the YouTube Series: https://pytorch.org/tutorials/beginner/introyt.html\n",
    "- You have some experience with PyTorch already and want to fully dive into the code? Awesome! Just clone or fork the code and do whatever you want with it. The code should be quite self-explanatory but if you have any questions, do not hesitate to ask me (Meike). As the creator of the code I know all the ins and outs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Where to look for what?**\n",
    "Let's point you to some parts of the PIP-Net code that you might want to adapt (if you want to do a more technical project). \n",
    "\n",
    "Load the model checkpoint by running main.py with argument ``--state_dict_dir_net path/to/checkpoint/net_trained``. Uncomment all lines in main.py that you don't need (basically any at this point). \n",
    "\n",
    "The weights from prototypes to classes is saved in a tensor called ``net.module._classification.weight`` (a tensor is similar to a numpy array, see https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py). If you want to change how much a prototype should add evidence to a class, you can change the values in ``net.module._classification.weight``. For example, setting the value at index ``i,j`` in ``net.module._classification.weight`` to 0, would mean that prototype ``i`` is disabled for class ``j``. Disabling of prototypes to fix undesired behaviour is also done in this paper: https://link.springer.com/chapter/10.1007/978-3-031-50396-2_11\n",
    "\n",
    "A local explanation is generated with ``util/visualize_prediction.py``. It explains for one single image the prediction. Run line 321-325 in main.py to get local explanations of images in the test set. Adapting this code could improve the presentation to the user. This code can also be used to check what happens when the weight of a prototype is changed. \n",
    "\n",
    "A global explanation of PIP-Net is generated with ``util/vis_pipnet.py``. I've already visualised them for you in two folders: visualised_prototypes and visualised_prototypes_topk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
